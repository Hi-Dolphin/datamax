import os
import re
import sys
import json
import asyncio
import requests
from pathlib import Path
from loguru import logger
from PIL import Image
import shutil

project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from datamax.crawler import ArxivCrawler
from datamax.parser import DataMax
from datamax.generator.multimodal_qa_generator import generatr_qa_pairs as generate_multimodal_qa_pairs
# from datamax.evaluator import MultimodalConsistencyEvaluator, TextQualityEvaluator


class PipelineConfig:
    SEARCH_QUERY = "intermodality shipping"
    MAX_PAPERS_TO_CRAWL = 1

    DASHSCOPE_API_KEY = os.getenv("DASHSCOPE_API_KEY", "sk-61139bb843e543d9b261c0b366f80c9e")
    DASHSCOPE_BASE_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1"
    QA_MODEL_NAME = "qwen-vl-plus" # Must be VLLM model

    QUESTIONS_PER_CHUNK = 2
    MAX_WORKERS = 4

    CLIP_SCORE_THRESHOLD = 0.2

    OUTPUT_BASE_DIR = Path("/workspace/volume/datasets-dev/intermodality/eva_multimodal")
    RAW_DATA_DIR = OUTPUT_BASE_DIR / "01_raw_data"
    PARSED_MD_DIR = OUTPUT_BASE_DIR / "02_parsed_markdown"
    GENERATED_QA_DIR = OUTPUT_BASE_DIR / "03_generated_qa"
    EVALUATED_DATA_DIR = OUTPUT_BASE_DIR / "04_evaluated_data"


class IntermodalityPipeline:
    """
    ä¸€ä¸ªå®Œæ•´çš„å¤šæ¨¡æ€æ•°æ®å¤„ç†æµæ°´çº¿ï¼ŒåŒ…æ‹¬ï¼š
    æŠ“å– -> è§£æ -> ç”Ÿæˆ -> è¯„ä¼°
    """
    def __init__(self, config: PipelineConfig):
        self.config = config
        # self.evaluator = MultimodalConsistencyEvaluator()
        self._setup_directories()

    def _setup_directories(self):
        """åˆ›å»ºæ‰€æœ‰éœ€è¦çš„è¾“å‡ºç›®å½•"""
        logger.info(f"ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨äº: {self.config.OUTPUT_BASE_DIR}")
        for path in [self.config.RAW_DATA_DIR, self.config.PARSED_MD_DIR,
                     self.config.GENERATED_QA_DIR, self.config.EVALUATED_DATA_DIR]:
            path.mkdir(parents=True, exist_ok=True)

    async def step_1_crawl_data(self) -> list[Path]:
        """æ­¥éª¤ 1: ä» ArXiv æŠ“å–å…³äºå¤šå¼è”è¿çš„è®ºæ–‡å¹¶ä¸‹è½½çœŸå®çš„PDFæ–‡ä»¶"""
        logger.info("--- æ­¥éª¤ 1: å¼€å§‹æŠ“å–å¹¶ä¸‹è½½çœŸå®PDFæ•°æ® ---")
        crawler = ArxivCrawler(config={'storage': {'base_path': str(self.config.RAW_DATA_DIR)}})
        
        try:
            logger.info(f"æ­£åœ¨æœç´¢ ArXiv, å…³é”®è¯: '{self.config.SEARCH_QUERY}'...")
            search_results = await crawler.crawl_async(
                self.config.SEARCH_QUERY,
                max_results=self.config.MAX_PAPERS_TO_CRAWL
            )
            
            if not search_results or not search_results.get('data'):
                logger.warning("æœªèƒ½ä» ArXiv æŠ“å–åˆ°ä»»ä½•è®ºæ–‡ã€‚")
                return []

            papers = search_results['data']
            logger.success(f"æˆåŠŸæ‰¾åˆ° {len(papers)} ç¯‡ç›¸å…³è®ºæ–‡ã€‚ç°åœ¨å¼€å§‹ä¸‹è½½...")
            
            downloaded_pdf_paths = []
            for paper in papers:
                pdf_url = paper.get('pdf_url')
                arxiv_id = paper.get('arxiv_id', 'unknown_id').replace('.', '_')
                
                if not pdf_url:
                    logger.warning(f"è®ºæ–‡ '{arxiv_id}' æ²¡æœ‰æä¾›PDFé“¾æ¥ï¼Œè·³è¿‡ã€‚")
                    continue

                pdf_filename = f"{arxiv_id}.pdf"
                local_pdf_path = self.config.RAW_DATA_DIR / pdf_filename
                
                try:
                    # Download the actual PDF file
                    logger.info(f"æ­£åœ¨ä¸‹è½½: {pdf_url} -> {local_pdf_path}")
                    response = requests.get(pdf_url, stream=True, timeout=30)
                    response.raise_for_status() # Will raise an HTTPError for bad responses (4xx or 5xx)

                    with open(local_pdf_path, 'wb') as f:
                        for chunk in response.iter_content(chunk_size=8192):
                            f.write(chunk)
                    
                    downloaded_pdf_paths.append(local_pdf_path)
                    logger.success(f"æˆåŠŸä¸‹è½½è®ºæ–‡PDFåˆ°: {local_pdf_path}")

                except requests.exceptions.RequestException as e:
                    logger.error(f"ä¸‹è½½ '{pdf_url}' å¤±è´¥: {e}")
                except Exception as e:
                    logger.error(f"å¤„ç† '{local_pdf_path}' æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")

            return downloaded_pdf_paths
            
        except Exception as e:
            logger.error(f"æ•°æ®æŠ“å–æ­¥éª¤å¤±è´¥: {e}")
            return []


    def step_2_parse_to_multimodal_markdown(self, pdf_paths: list[Path]) -> list[Path]:
        """æ­¥éª¤ 2: å°†PDFè§£æä¸ºåŒ…å«å›¾ç‰‡å’Œæ–‡æœ¬çš„Markdownæ–‡ä»¶"""
        logger.info("--- æ­¥éª¤ 2: å¼€å§‹è§£æPDFä¸ºå¤šæ¨¡æ€Markdown ---")
        md_paths = []
        for pdf_path in pdf_paths:
            try:
                logger.info(f"æ­£åœ¨è§£æ: {pdf_path.name}")
                dm = DataMax(
                    file_path=str(pdf_path), 
                    use_qwen_vl_ocr=True,
                    api_key=self.config.DASHSCOPE_API_KEY,
                    base_url=self.config.DASHSCOPE_BASE_URL,
                    model_name=self.config.QA_MODEL_NAME
                )
                parsed_result = dm.get_data()

                if not parsed_result or not parsed_result.get('content'):
                    logger.warning(f"è§£æå¤±è´¥æˆ–å†…å®¹ä¸ºç©º: {pdf_path.name}")
                    continue

                md_content = parsed_result['content']
                md_filename = pdf_path.stem + ".md"
                md_path = self.config.PARSED_MD_DIR / md_filename


                relative_image_dir = Path("images") / pdf_path.stem
                
                def path_replacer(match):
                    image_filename = match.group(1)
                    new_path = (relative_image_dir / image_filename).as_posix()
                    return f"![image]({new_path})"

                image_pattern = r'!\[[^\]]*\]\(([^)]+)\)'
                md_content_corrected = re.sub(image_pattern, path_replacer, md_content)
                
                logger.info("å·²ä¿®æ­£Markdownæ–‡ä»¶ä¸­çš„å›¾ç‰‡ç›¸å¯¹è·¯å¾„ã€‚")
                
                with open(md_path, "w", encoding="utf-8") as f:
                    f.write(md_content)
                
                md_paths.append(md_path)
                logger.success(f"æˆåŠŸå°† '{pdf_path.name}' è§£æå¹¶ä¿å­˜åˆ° '{md_path}'")
                
                temp_image_dir = Path("__temp__") / "images" / pdf_path.stem
                if temp_image_dir.exists():
                    target_image_dir = self.config.PARSED_MD_DIR / "images" / pdf_path.stem
                    target_image_dir.mkdir(parents=True, exist_ok=True)
                    shutil.copytree(temp_image_dir, target_image_dir, dirs_exist_ok=True)
                    logger.info(f"å·²å°†ç›¸å…³å›¾ç‰‡ä» '{temp_image_dir}' å¤åˆ¶åˆ° '{target_image_dir}'")

            except Exception as e:
                logger.error(f"è§£ææ–‡ä»¶ '{pdf_path.name}' å¤±è´¥: {e}")
        
        return md_paths

    def step_3_generate_multimodal_qa(self, md_paths: list[Path]) -> list[dict]:
        """æ­¥éª¤ 3: ä»Markdownæ–‡ä»¶ç”Ÿæˆå¤šæ¨¡æ€é—®ç­”å¯¹"""
        logger.info("--- æ­¥éª¤ 3: å¼€å§‹ç”Ÿæˆå¤šæ¨¡æ€é—®ç­”å¯¹ ---")
        all_qa_pairs = []
        for md_path in md_paths:
            try:
                logger.info(f"æ­£åœ¨ä¸º '{md_path.name}' ç”Ÿæˆé—®ç­”å¯¹...")
                qa_pairs = generate_multimodal_qa_pairs(
                    file_path=str(md_path),
                    api_key=self.config.DASHSCOPE_API_KEY,
                    model_name=self.config.QA_MODEL_NAME,
                    question_number=self.config.QUESTIONS_PER_CHUNK,
                    max_workers=self.config.MAX_WORKERS,
                    debug=True
                )
                
                if not qa_pairs:
                    logger.warning(f"æœªèƒ½ä¸º '{md_path.name}' ç”Ÿæˆä»»ä½•é—®ç­”å¯¹ã€‚")
                    continue

                for pair in qa_pairs:
                    pair['source_file'] = md_path.name
                
                all_qa_pairs.extend(qa_pairs)
                
                output_path = self.config.GENERATED_QA_DIR / (md_path.stem + "_qa.json")
                with open(output_path, "w", encoding="utf-8") as f:
                    json.dump(qa_pairs, f, indent=2, ensure_ascii=False)
                
                logger.success(f"æˆåŠŸä¸º '{md_path.name}' ç”Ÿæˆ {len(qa_pairs)} ä¸ªé—®ç­”å¯¹ï¼Œå·²ä¿å­˜ã€‚")
                
            except Exception as e:
                logger.error(f"ä¸º '{md_path.name}' ç”Ÿæˆé—®ç­”å¯¹å¤±è´¥: {e}")

        logger.info(f"æ€»å…±ç”Ÿæˆ {len(all_qa_pairs)} ä¸ªå¤šæ¨¡æ€é—®ç­”å¯¹ã€‚")
        return all_qa_pairs

    # def step_4_evaluate_and_filter(self, generated_qa: list[dict]) -> list[dict]:
    #     """æ­¥éª¤ 4: ä½¿ç”¨é‡åŒ–æŒ‡æ ‡è¯„ä¼°å¹¶ç­›é€‰ç”Ÿæˆçš„é—®ç­”å¯¹"""
    #     logger.info("--- æ­¥éª¤ 4: å¼€å§‹è¯„ä¼°å’Œç­›é€‰æ•°æ® ---")
    #     high_quality_data = []
    #     evaluation_report = []

    #     if not generated_qa:
    #         logger.warning("æ²¡æœ‰å¯ä¾›è¯„ä¼°çš„æ•°æ®ã€‚")
    #         return [], {}

    #     logger.info(f"è¯„ä¼°é˜ˆå€¼ (CLIP Score) > {self.config.CLIP_SCORE_THRESHOLD}")
        
    #     for i, qa_item in enumerate(generated_qa):
    #         user_message = qa_item['messages'][0]['content']
    #         assistant_message = qa_item['messages'][1]['content']
    #         images = qa_item['images']
            
    #         if not images:
    #             logger.warning(f"é—®ç­”å¯¹ {i+1} ç¼ºå°‘å›¾ç‰‡ï¼Œè·³è¿‡è¯„ä¼°ã€‚")
    #             continue

    #         # ç›®å‰çš„ç”Ÿæˆé€»è¾‘æ˜¯ä¸€ä¸ªQAå¯¹å…³è”ä¸€ä¸ªæˆ–å¤šä¸ªå›¾ç‰‡ï¼Œæˆ‘ä»¬ä¸»è¦è¯„ä¼°å’Œç¬¬ä¸€å¼ å›¾çš„ä¸€è‡´æ€§
    #         image_path = images[0]
            
    #         try:
    #             # è¯„ä¼°é—®é¢˜ä¸å›¾ç‰‡çš„ä¸€è‡´æ€§
    #             question_text = user_message.replace("<image>", "").strip()
    #             clip_score_q = self.evaluator.evaluate_clip_score(image_path, question_text)
                
    #             # è¯„ä¼°ç­”æ¡ˆä¸å›¾ç‰‡çš„ä¸€è‡´æ€§
    #             clip_score_a = self.evaluator.evaluate_clip_score(image_path, assistant_message)

    #             similarity_q = clip_score_q.get("cosine_similarity", 0)
    #             similarity_a = clip_score_a.get("cosine_similarity", 0)
    #             avg_similarity = (similarity_q + similarity_a) / 2

    #             report_entry = {
    #                 "qa_index": i + 1,
    #                 "source_file": qa_item.get('source_file'),
    #                 "image": image_path,
    #                 "question": question_text,
    #                 "answer": assistant_message,
    #                 "question_clip_score": similarity_q,
    #                 "answer_clip_score": similarity_a,
    #                 "average_clip_score": avg_similarity,
    #                 "passed": avg_similarity > self.config.CLIP_SCORE_THRESHOLD
    #             }
    #             evaluation_report.append(report_entry)

    #             if report_entry["passed"]:
    #                 qa_item['evaluation_scores'] = {
    #                     "question_clip_score": similarity_q,
    #                     "answer_clip_score": similarity_a,
    #                     "average_clip_score": avg_similarity
    #                 }
    #                 high_quality_data.append(qa_item)
    #                 logger.debug(f"QA #{i+1} é€šè¿‡è¯„ä¼°, å¹³å‡åˆ†: {avg_similarity:.4f}")
    #             else:
    #                 logger.debug(f"QA #{i+1} æœªé€šè¿‡è¯„ä¼°, å¹³å‡åˆ†: {avg_similarity:.4f}")

    #         except Exception as e:
    #             logger.error(f"è¯„ä¼°é—®ç­”å¯¹ #{i+1} æ—¶å‡ºé”™: {e}")
    #             evaluation_report.append({"qa_index": i + 1, "error": str(e)})

    #     report_path = self.config.EVALUATED_DATA_DIR / "evaluation_report.json"
    #     with open(report_path, "w", encoding="utf-8") as f:
    #         json.dump(evaluation_report, f, indent=2, ensure_ascii=False)
    #     logger.info(f"è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
        
    #     final_data_path = self.config.EVALUATED_DATA_DIR / "high_quality_multimodal_data.jsonl"
    #     with open(final_data_path, "w", encoding="utf-8") as f:
    #         for item in high_quality_data:
    #             f.write(json.dumps(item, ensure_ascii=False) + "\n")
    #     logger.success(f"ç­›é€‰å‡ºçš„ {len(high_quality_data)} æ¡é«˜è´¨é‡æ•°æ®å·²ä¿å­˜åˆ°: {final_data_path}")
        
    #     return high_quality_data, evaluation_report

    async def run(self):
        logger.info("ğŸš€ å¯åŠ¨å¤šå¼è”è¿å¤šæ¨¡æ€æ•°æ®ç”Ÿæˆä¸éªŒè¯æµæ°´çº¿...")
        
        if self.config.DASHSCOPE_API_KEY == "YOUR_DASHSCOPE_API_KEY":
            logger.error("è¯·åœ¨è„šæœ¬é¡¶éƒ¨æˆ–ç¯å¢ƒå˜é‡ä¸­è®¾ç½®æ‚¨çš„ DASHSCOPE_API_KEYã€‚")
            return

        pdf_files = await self.step_1_crawl_data()
        if not pdf_files:
            logger.error("æµæ°´çº¿å› æ•°æ®æŠ“å–å¤±è´¥è€Œç»ˆæ­¢ã€‚")
            return

        md_files = self.step_2_parse_to_multimodal_markdown(pdf_files)
        if not md_files:
            logger.error("æµæ°´çº¿å› PDFè§£æå¤±è´¥è€Œç»ˆæ­¢ã€‚")
            return

        generated_qa = self.step_3_generate_multimodal_qa(md_files)
        if not generated_qa:
            logger.error("æµæ°´çº¿å› æœªèƒ½ç”Ÿæˆä»»ä½•é—®ç­”å¯¹è€Œç»ˆæ­¢ã€‚")
            return

        # final_data, report = self.step_4_evaluate_and_filter(generated_qa)

        logger.info("--- æµæ°´çº¿æ‰§è¡Œæ€»ç»“ ---")
        total_generated = len(generated_qa)
        # total_passed = len(final_data)
        # pass_rate = (total_passed / total_generated * 100) if total_generated > 0 else 0
        logger.success(f"æ€»å…±ç”Ÿæˆ {total_generated} ä¸ªé—®ç­”å¯¹ã€‚")
        # logger.success(f"ç»è¿‡è¯„ä¼°ï¼Œç­›é€‰å‡º {total_passed} ä¸ªé«˜è´¨é‡é—®ç­”å¯¹ (é€šè¿‡ç‡: {pass_rate:.2f}%)ã€‚")
        logger.success(f"æœ€ç»ˆæ•°æ®é›†ä¿å­˜åœ¨: {self.config.EVALUATED_DATA_DIR}")
        logger.info("ğŸ‰ æµæ°´çº¿æ‰§è¡Œå®Œæ¯•ï¼")


if __name__ == "__main__":
    pipeline = IntermodalityPipeline(config=PipelineConfig())
    asyncio.run(pipeline.run())