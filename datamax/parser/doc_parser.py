import html
import os
import re
import shlex
import shutil
import subprocess
import tempfile
from pathlib import Path

import chardet
from loguru import logger

from datamax.parser.base import BaseLife, MarkdownOutputVo
from datamax.utils.lifecycle_types import LifeType

# Try to import OLE-related libraries (for reading DOC internal structure)
try:
    import olefile

    HAS_OLEFILE = True
except ImportError:
    HAS_OLEFILE = False
    logger.warning("‚ö†Ô∏è olefile library not installed, advanced DOC parsing unavailable")

# Try to import UNO processor
try:
    from datamax.utils.uno_handler import HAS_UNO, convert_with_uno
except ImportError:
    HAS_UNO = False
    logger.error(
        "‚ùå UNO processor import failed!\n"
        "üîß Solution:\n"
        "1. Install LibreOffice and python-uno:\n"
        "   - Ubuntu/Debian: sudo apt-get install libreoffice python3-uno\n"
        "   - CentOS/RHEL: sudo yum install libreoffice python3-uno\n"
        "   - macOS: brew install libreoffice\n"
        "   - Windows: Download and install LibreOffice\n"
        "2. Ensure Python can access uno module:\n"
        "   - Linux: export PYTHONPATH=/usr/lib/libreoffice/program:$PYTHONPATH\n"
        "   - Windows: Add LibreOffice\\program to system PATH\n"
        "3. Verify installation: python -c 'import uno'\n"
        "4. If issues persist, see full documentation:\n"
        "   https://wiki.documentfoundation.org/Documentation/DevGuide/Installing_the_SDK"
    )


class DocParser(BaseLife):
    def __init__(
        self,
        file_path: str | list,
        to_markdown: bool = False,
        use_uno: bool = True,
        domain: str = "Technology",
    ):
        super().__init__(domain=domain)
        self.file_path = file_path
        self.to_markdown = to_markdown

        # Prioritize UNO (unless explicitly disabled)
        if use_uno and HAS_UNO:
            self.use_uno = True
            logger.info(
                "üöÄ DocParser initialized - using UNO API for single-threaded efficient processing"
            )
        else:
            self.use_uno = False
            if use_uno and not HAS_UNO:
                logger.warning(
                    "‚ö†Ô∏è UNO unavailable, falling back to traditional command line approach\n"
                    "üí° Tip: UNO conversion is faster and more stable, strongly recommend installing and configuring UNO\n"
                    "üìñ Please refer to installation guide in error message above"
                )
            else:
                logger.info(
                    "üöÄ DocParser initialized - using traditional command line approach"
                )

        logger.info(f"üìÑ File path: {file_path}, convert to markdown: {to_markdown}")

    def extract_all_content(self, doc_path: str) -> str:
        """
        Comprehensively extract all content from DOC files
        Supports multiple DOC internal formats and storage methods
        """
        logger.info(f"üîç Starting comprehensive content extraction: {doc_path}")

        all_content = []

        try:
            # 1. Try to extract content using OLE parsing (if available)
            if HAS_OLEFILE:
                ole_content = self._extract_ole_content(doc_path)
                if ole_content:
                    all_content.append(("ole", ole_content))

            # 2. Try to extract embedded objects
            embedded_content = self._extract_embedded_objects(doc_path)
            if embedded_content:
                all_content.append(("embedded", embedded_content))

            # 3. If none of the above methods extracted content, use traditional conversion
            if not all_content:
                logger.info("üîÑ Using traditional conversion method to extract content")
                return ""  # Return empty, let caller use traditional method

            # Check content quality, especially for WPS files
            for content_type, content in all_content:
                if content and self._check_content_quality(content):
                    logger.info(
                        f"‚úÖ Successfully extracted content using {content_type}"
                    )
                    return content

            # If all content quality is poor, return empty
            logger.warning("‚ö†Ô∏è All extraction methods produced poor quality content")
            return ""

        except Exception as e:
            logger.error(f"üí• Comprehensive content extraction failed: {str(e)}")
            return ""

    def _is_wps_file(self, streams: list) -> bool:
        """Check if the OLE file is generated by WPS Office"""
        return any("WpsCustomData" in str(stream) for stream in streams)

    def _extract_word_document(self, ole) -> str:
        """Extract text from WordDocument stream"""
        if not ole.exists("WordDocument"):
            return ""

        try:
            word_stream = ole.openstream("WordDocument").read()
            logger.info(f"üìÑ WordDocument stream size: {len(word_stream)} bytes")

            text = self._parse_word_stream(word_stream)
            return text or ""

        except Exception as e:
            logger.error(f"üí• Failed to parse WordDocument stream: {str(e)}")
            return ""

    def _extract_additional_streams(self, ole) -> str:
        """Extract text from other OLE streams that may contain text"""
        text_content = []

        for entry in ole.listdir():
            if not any(name in str(entry) for name in ["Text", "Content", "Body"]):
                continue

            try:
                stream = ole.openstream(entry)
                data = stream.read()
                decoded = self._try_decode_bytes(data)

                if decoded and len(decoded.strip()) > 10:
                    text_content.append(decoded)

            except Exception:
                continue

        if not text_content:
            return ""

        combined = "\n".join(text_content)
        logger.info(f"üìÑ Extracted text from OLE streams: {len(combined)} characters")
        return self._clean_extracted_text(combined)

    def _extract_ole_content(self, doc_path: str) -> str:
        """Extract DOC content using OLE parsing"""
        try:
            ole = olefile.OleFileIO(doc_path)
            logger.info(f"üìÇ Successfully opened OLE file: {doc_path}")

            streams = ole.listdir()
            logger.debug(f"üìã Available OLE streams: {streams}")

            # Detect WPS DOC
            if self._is_wps_file(streams):
                logger.info(
                    "üìù Detected WPS DOC file, using traditional conversion instead"
                )
                ole.close()
                return ""

            # Extract WordDocument stream
            word_text = self._extract_word_document(ole)
            if word_text:
                return word_text

            # Extract from secondary streams
            extra_text = self._extract_additional_streams(ole)
            ole.close()

            return extra_text or ""

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è OLE parsing failed: {str(e)}")
            return ""

    def _parse_word_stream(self, data: bytes) -> str:
        """Parse text from WordDocument stream"""
        try:
            # DOC file format is complex, this provides basic text extraction
            # Look for text fragments
            text_parts = []

            # Try multiple encodings, especially for Chinese text
            for encoding in [
                "utf-16-le",
                "utf-8",
                "gbk",
                "gb18030",
                "gb2312",
                "big5",
                "cp936",
                "cp1252",
            ]:
                try:
                    decoded = data.decode(encoding, errors="ignore")

                    # Count Chinese characters
                    chinese_chars = sum(1 for c in decoded if "\u4e00" <= c <= "\u9fff")

                    if chinese_chars > 10 or (decoded and len(decoded.strip()) > 50):
                        # Filter printable characters, but keep Chinese
                        cleaned = self._filter_printable_text(decoded)
                        if cleaned and len(cleaned.strip()) > 20:
                            text_parts.append(cleaned)
                            logger.debug(
                                f"üìù Successfully decoded using {encoding}, "
                                f"contains {chinese_chars} Chinese characters"
                            )
                            break

                except Exception:
                    continue

            return "\n".join(text_parts) if text_parts else ""

        except Exception as e:
            logger.error(f"üí• Failed to parse Word stream: {str(e)}")
            return ""

    def _filter_printable_text(self, text: str) -> str:
        """Filter text, keeping printable characters and Chinese"""
        result = []
        for char in text:
            # Keep Chinese characters
            if "\u4e00" <= char <= "\u9fff":
                result.append(char)
            # Keep Japanese characters
            elif "\u3040" <= char <= "\u30ff":
                result.append(char)
            # Keep Korean characters
            elif "\uac00" <= char <= "\ud7af":
                result.append(char)
            # Keep ASCII printable characters and whitespace
            elif char.isprintable() or char.isspace():
                result.append(char)
            # Keep common punctuation marks
            elif char in 'Ôºå„ÄÇÔºÅÔºüÔºõÔºö""' "ÔºàÔºâ„Äê„Äë„Ää„Äã„ÄÅ¬∑‚Ä¶‚Äî":
                result.append(char)

        return "".join(result)

    def _try_decode_bytes(self, data: bytes) -> str:
        """Try to decode byte data using multiple encodings"""
        # Prioritize Chinese encodings
        encodings = [
            "utf-8",
            "gbk",
            "gb18030",
            "gb2312",
            "big5",
            "utf-16-le",
            "utf-16-be",
            "cp936",
            "cp1252",
            "latin-1",
        ]

        # First try to detect encoding using chardet
        try:
            import chardet

            detected = chardet.detect(data)
            if detected["encoding"] and detected["confidence"] > 0.7:
                encodings.insert(0, detected["encoding"])
                logger.debug(
                    f"üîç Detected encoding: {detected['encoding']} "
                    f"(confidence: {detected['confidence']})"
                )
        except Exception:
            pass

        for encoding in encodings:
            try:
                decoded = data.decode(encoding, errors="ignore")
                # Check if contains meaningful text (including Chinese)
                if decoded and (
                    any(c.isalnum() for c in decoded)
                    or any("\u4e00" <= c <= "\u9fff" for c in decoded)
                ):
                    # Further clean the text
                    cleaned = self._filter_printable_text(decoded)
                    if cleaned and len(cleaned.strip()) > 10:
                        return cleaned
            except Exception:
                continue

        return ""

    def _extract_embedded_objects(self, doc_path: str) -> str:
        """Extract embedded objects from DOC file"""
        try:
            if not HAS_OLEFILE:
                return ""

            embedded_content = []

            with olefile.OleFileIO(doc_path) as ole:
                # Look for embedded objects
                for entry in ole.listdir():
                    entry_name = "/".join(entry)

                    # Check if it's an embedded object
                    if any(
                        pattern in entry_name.lower()
                        for pattern in ["object", "embed", "package"]
                    ):
                        logger.info(f"üìé Found embedded object: {entry_name}")
                        try:
                            stream = ole.openstream(entry)
                            data = stream.read()

                            # Try to extract text content
                            text = self._try_decode_bytes(data)
                            if text and len(text.strip()) > 20:
                                embedded_content.append(text.strip())

                        except Exception as inner_exc:
                            logger.debug(
                                f"Could not extract embedded object '{entry_name}': {inner_exc}"
                            )
                            continue

            return "\n\n".join(embedded_content) if embedded_content else ""

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to extract embedded objects: {str(e)}")
            return ""

    def _clean_extracted_text(self, text: str) -> str:
        """Clean extracted text, thoroughly remove all XML tags and control characters, keep only plain text"""
        try:
            # 1. Decode HTML/XML entities
            text = html.unescape(text)

            # 2. Remove all XML/HTML tags
            text = re.sub(r"<[^>]+>", "", text)

            # 3. Remove XML namespace prefixes
            text = re.sub(r"\b\w+:", "", text)

            # 4. Remove NULL characters and other control characters
            text = re.sub(r"[\x00-\x08\x0b-\x0c\x0e-\x1f\x7f]", "", text)

            # 5. Remove special XML character sequences
            text = re.sub(r"&[a-zA-Z]+;", "", text)
            text = re.sub(r"&#\d+;", "", text)
            text = re.sub(r"&#x[0-9a-fA-F]+;", "", text)

            # 6. Keep meaningful characters, remove other special characters
            # Keep: Chinese, Japanese, Korean, English, numbers, common punctuation and whitespace
            allowed_chars = (
                r"\w\s"  # Letters, numbers and whitespace
                r"\u4e00-\u9fff"  # Chinese
                r"\u3040-\u30ff"  # Japanese
                r"\uac00-\ud7af"  # Korean
                r'Ôºå„ÄÇÔºÅÔºüÔºõÔºö""'
                "ÔºàÔºâ„Äê„Äë„Ää„Äã„ÄÅ¬∑‚Ä¶‚Äî"  # Chinese punctuation
                r'.,!?;:()[\]{}"\'`~@#$%^&*+=\-_/\\'  # English punctuation and common symbols
            )

            # Use stricter filtering, but keep all meaningful characters
            cleaned_text = "".join(
                char for char in text if re.match(f"[{allowed_chars}]", char)
            )

            # 7. Remove excessively long meaningless character sequences (usually binary garbage)
            cleaned_text = re.sub(r"([^\s\u4e00-\u9fff])\1{5,}", r"\1", cleaned_text)

            # 8. Clean excessive whitespace, but preserve paragraph structure
            cleaned_text = re.sub(
                r"[ \t]+", " ", cleaned_text
            )  # Multiple spaces/tabs become single space
            cleaned_text = re.sub(
                r"\n\s*\n\s*\n+", "\n\n", cleaned_text
            )  # Multiple empty lines become double empty lines
            cleaned_text = re.sub(
                r"^\s+|\s+$", "", cleaned_text, flags=re.MULTILINE
            )  # Remove leading/trailing whitespace

            # 9. Further cleaning: remove standalone punctuation lines
            lines = cleaned_text.split("\n")
            cleaned_lines = []

            for line in lines:
                line = line.strip()
                if line:
                    # Check if line is mainly meaningful content
                    # Calculate ratio of Chinese, English letters and numbers
                    meaningful_chars = sum(
                        1 for c in line if (c.isalnum() or "\u4e00" <= c <= "\u9fff")
                    )

                    # Keep if meaningful chars ratio > 30%, or line length < 5 (might be title)
                    if len(line) < 5 or (
                        meaningful_chars > 0 and meaningful_chars / len(line) > 0.3
                    ):
                        cleaned_lines.append(line)
                elif cleaned_lines and cleaned_lines[-1]:  # Keep paragraph separators
                    cleaned_lines.append("")

            result = "\n".join(cleaned_lines).strip()

            # 10. Final check
            if len(result) < 10:
                logger.warning("‚ö†Ô∏è Cleaned text too short, may have issues")
                return ""

            # Check if still contains XML tags
            if re.search(r"<[^>]+>", result):
                logger.warning(
                    "‚ö†Ô∏è Cleaned text still contains XML tags, performing second cleanup"
                )
                result = re.sub(r"<[^>]+>", "", result)

            return result

        except Exception as e:
            logger.error(f"üí• Failed to clean text: {str(e)}")
            return text

    def _combine_extracted_content(self, content_list: list) -> str:
        """Combine various extracted content"""
        combined = []

        # Sort content by priority
        priority_order = ["ole", "embedded", "converted", "fallback"]

        for content_type in priority_order:
            for item_type, content in content_list:
                if item_type == content_type and content.strip():
                    combined.append(content.strip())

        # Add other uncategorized content
        for item_type, content in content_list:
            if item_type not in priority_order and content.strip():
                combined.append(content.strip())

        return "\n\n".join(combined) if combined else ""

    def doc_to_txt(self, doc_path: str, dir_path: str) -> str:
        """Convert .doc file to .txt file"""
        logger.info(
            f"üîÑ Starting DOC to TXT conversion - source file: {doc_path}, output directory: {dir_path}"
        )

        if self.use_uno:
            # Use UNO API for conversion
            try:
                logger.info("üéØ Using UNO API for document conversion...")
                txt_path = convert_with_uno(doc_path, "txt", dir_path)

                if not os.path.exists(txt_path):
                    logger.error(f"‚ùå Converted TXT file does not exist: {txt_path}")
                    raise Exception(f"File conversion failed {doc_path} ==> {txt_path}")
                else:
                    logger.info(
                        f"üéâ TXT file conversion successful, file path: {txt_path}"
                    )
                    return txt_path

            except Exception as e:
                logger.error(
                    f"üí• UNO conversion failed: {str(e)}\n"
                    f"üîç Diagnostic information:\n"
                    f"   - Error type: {type(e).__name__}\n"
                    f"   - Is LibreOffice installed? Try running: soffice --version\n"
                    f"   - Is Python UNO module available? Try: python -c 'import uno'\n"
                    f"   - Are there other LibreOffice instances running?\n"
                    f"   - Are file permissions correct?\n"
                    f"üîß Possible solutions:\n"
                    f"   1. Ensure LibreOffice is properly installed\n"
                    f"   2. Close all LibreOffice processes\n"
                    f"   3. Check file permissions and paths\n"
                    f'   4. Try manual run: soffice --headless --convert-to txt "{doc_path}"'
                )
                logger.warning(
                    "‚ö†Ô∏è Automatically falling back to traditional command line method..."
                )
                return self._doc_to_txt_subprocess(doc_path, dir_path)
        else:
            # Use traditional subprocess method
            return self._doc_to_txt_subprocess(doc_path, dir_path)

    def _doc_to_txt_subprocess(self, doc_path: str, dir_path: str) -> str:
        """Convert .doc file to .txt file using subprocess (traditional method)"""
        try:
            cmd = [
                "soffice",
                "--headless",
                "--convert-to",
                "txt",
                str(doc_path),
                "--outdir",
                str(dir_path),
            ]
            cmd_display = " ".join(shlex.quote(part) for part in cmd)
            logger.debug(f"‚ö° Executing conversion command: {cmd_display}")

            process = subprocess.Popen(
                cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE
            )
            stdout, stderr = process.communicate()
            exit_code = process.returncode

            if exit_code == 0:
                logger.info(
                    f"‚úÖ DOC to TXT conversion successful - exit code: {exit_code}"
                )
                if stdout:
                    logger.debug(
                        f"üìÑ Conversion output: {stdout.decode('utf-8', errors='replace')}"
                    )
            else:
                encoding = chardet.detect(stderr)["encoding"]
                if encoding is None:
                    encoding = "utf-8"
                error_msg = stderr.decode(encoding, errors="replace")
                logger.error(
                    f"‚ùå DOC to TXT conversion failed - exit code: {exit_code}, error message: {error_msg}"
                )
                raise Exception(
                    f"Error Output (detected encoding: {encoding}): {error_msg}"
                )

            fname = str(Path(doc_path).stem)
            txt_path = os.path.join(dir_path, f"{fname}.txt")

            if not os.path.exists(txt_path):
                logger.error(f"‚ùå Converted TXT file does not exist: {txt_path}")
                raise Exception(f"File conversion failed {doc_path} ==> {txt_path}")
            else:
                logger.info(f"üéâ TXT file conversion successful, file path: {txt_path}")
                return txt_path

        except subprocess.SubprocessError as e:
            logger.error(f"üí• subprocess execution failed: {str(e)}")
            raise Exception(
                f"Error occurred while executing conversion command: {str(e)}"
            )
        except Exception as e:
            logger.error(
                f"üí• Unknown error occurred during DOC to TXT conversion: {str(e)}"
            )
            raise

    def read_txt_file(self, txt_path: str) -> str:
        """Read txt file content"""
        logger.info(f"üìñ Starting to read TXT file: {txt_path}")

        try:
            # Detect file encoding
            with open(txt_path, "rb") as f:
                raw_data = f.read()
                encoding = chardet.detect(raw_data)["encoding"]
                if encoding is None:
                    encoding = "utf-8"
                logger.debug(f"üîç Detected file encoding: {encoding}")

            # Read file content
            with open(txt_path, "r", encoding=encoding, errors="replace") as f:
                content = f.read()

            logger.info(
                f"üìÑ TXT file reading complete - content length: {len(content)} characters"
            )
            logger.debug(f"üëÄ First 100 characters preview: {content[:100]}...")

            return content

        except FileNotFoundError as e:
            logger.error(f"üö´ TXT file not found: {str(e)}")
            raise Exception(f"File not found: {txt_path}")
        except Exception as e:
            logger.error(f"üí• Error occurred while reading TXT file: {str(e)}")
            raise

    def read_doc_file(self, doc_path: str) -> str:
        """Read doc file and convert to text"""
        logger.info(f"üìñ Starting to read DOC file - file: {doc_path}")

        try:
            # First try comprehensive extraction (if advanced parsing features available)
            if HAS_OLEFILE:
                comprehensive_content = self.extract_all_content(doc_path)
                if comprehensive_content and comprehensive_content.strip():
                    # Check content quality
                    if self._check_content_quality(comprehensive_content):
                        logger.info(
                            f"‚ú® Comprehensive extraction successful, content length: {len(comprehensive_content)} characters"
                        )
                        return comprehensive_content
                    else:
                        logger.warning(
                            "‚ö†Ô∏è Comprehensive extraction content quality poor, trying other methods"
                        )

            # Fallback to traditional conversion method
            logger.info("üîÑ Using traditional conversion method")

            with tempfile.TemporaryDirectory() as temp_path:
                logger.debug(f"üìÅ Created temporary directory: {temp_path}")

                temp_dir = Path(temp_path)

                file_path = temp_dir / "tmp.doc"
                shutil.copy(doc_path, file_path)
                logger.debug(
                    f"üìã Copied file to temporary directory: {doc_path} -> {file_path}"
                )

                # Convert DOC to TXT
                txt_file_path = self.doc_to_txt(str(file_path), str(temp_path))
                logger.info(f"üéØ DOC to TXT conversion complete: {txt_file_path}")

                # Read TXT file content
                content = self.read_txt_file(txt_file_path)
                logger.info(
                    f"‚ú® TXT file content reading complete, content length: {len(content)} characters"
                )

                return content

        except FileNotFoundError as e:
            logger.error(f"üö´ File not found: {str(e)}")
            raise Exception(f"File not found: {doc_path}")
        except PermissionError as e:
            logger.error(f"üîí File permission error: {str(e)}")
            raise Exception(f"No permission to access file: {doc_path}")
        except Exception as e:
            logger.error(f"üí• Error occurred while reading DOC file: {str(e)}")
            raise

    def _check_content_quality(self, content: str) -> bool:
        """Check quality of extracted content"""
        if not content or len(content) < 50:
            return False

        # Calculate ratio of garbled characters
        total_chars = len(content)
        # Recognizable characters: ASCII, Chinese, Japanese, Korean, common punctuation
        recognizable = sum(
            1
            for c in content
            if (
                c.isascii()
                or "\u4e00" <= c <= "\u9fff"
                or "\u3040" <= c <= "\u30ff"
                or "\uac00" <= c <= "\ud7af"
                or c in 'Ôºå„ÄÇÔºÅÔºüÔºõÔºö""' "ÔºàÔºâ„Äê„Äë„Ää„Äã„ÄÅ¬∑‚Ä¶‚Äî\n\r\t "
            )  # Chinese  # Japanese  # Korean
        )

        # If recognizable character ratio < 70%, consider quality poor
        if recognizable / total_chars < 0.7:
            logger.warning(
                f"‚ö†Ô∏è Content quality check failed: recognizable character ratio {recognizable}/{total_chars} = {recognizable / total_chars:.2%}"
            )
            return False

        return True

    def parse(self, file_path: str):
        """Parse DOC file"""
        logger.info(f"üé¨ Starting to parse DOC file: {file_path}")

        try:
            # Verify file exists
            if not os.path.exists(file_path):
                logger.error(f"üö´ File does not exist: {file_path}")
                raise FileNotFoundError(f"File does not exist: {file_path}")

            # Verify file extension
            if not file_path.lower().endswith(".doc"):
                logger.warning(f"‚ö†Ô∏è File extension is not .doc: {file_path}")

            # Verify file size
            file_size = os.path.getsize(file_path)
            logger.info(f"üìè File size: {file_size} bytes")

            if file_size == 0:
                logger.warning(f"‚ö†Ô∏è File size is 0 bytes: {file_path}")
            # Lifecycle: Data Processing start
            lc_start = self.generate_lifecycle(
                source_file=file_path,
                domain=self.domain,
                life_type=LifeType.DATA_PROCESSING,
                usage_purpose="Documentation",
            )

            # üè∑Ô∏è Extract file extension
            extension = self.get_file_extension(file_path)
            logger.debug(f"üè∑Ô∏è Extracted file extension: {extension}")

            # Read file content
            logger.info("üìù Reading DOC file content")
            content = self.read_doc_file(doc_path=file_path)

            # Decide whether to keep original format or process as markdown based on to_markdown parameter
            if self.to_markdown:
                # Simple text to markdown conversion (preserve paragraph structure)
                mk_content = self.format_as_markdown(content)
                logger.info("üé® Content formatted as markdown")
            else:
                mk_content = content
                logger.info("üìù Keeping original text format")
            # 3) Lifecycle: Data Processed or Failed
            lc_end = self.generate_lifecycle(
                source_file=file_path,
                domain=self.domain,
                life_type=(
                    LifeType.DATA_PROCESSED
                    if mk_content.strip()
                    else LifeType.DATA_PROCESS_FAILED
                ),
                usage_purpose="Documentation",
            )

            logger.info(
                f"üéä File content parsing complete, final content length: {len(mk_content)} characters"
            )

            # Check if content is empty
            if not mk_content.strip():
                logger.warning(f"‚ö†Ô∏è Parsed content is empty: {file_path}")

            lifecycle = self.generate_lifecycle(
                source_file=file_path,
                domain=self.domain,
                usage_purpose="Documentation",
                life_type="LLM_ORIGIN",
            )
            logger.debug(f"‚öôÔ∏è Lifecycle information generation complete: {lifecycle}")

            output_vo = MarkdownOutputVo(extension, mk_content)
            output_vo.add_lifecycle(lc_start)
            output_vo.add_lifecycle(lc_end)
            # output_vo.add_lifecycle(lc_origin)

            result = output_vo.to_dict()
            logger.info(f"üèÜ DOC file parsing complete: {file_path}")
            logger.debug(f"üîë Return result keys: {list(result.keys())}")

            return result

        except FileNotFoundError as e:
            logger.error(f"üö´ File not found error: {str(e)}")
            raise
        except PermissionError as e:
            logger.error(f"üîí File permission error: {str(e)}")
            raise Exception(f"No permission to access file: {file_path}")
        except Exception as e:
            logger.error(
                f"üíÄ Failed to parse DOC file: {file_path}, error type: {type(e).__name__}, error message: {str(e)}"
            )
            raise

    def format_as_markdown(self, content: str) -> str:
        """Format plain text as simple markdown"""
        if not content.strip():
            return content

        lines = content.split("\n")
        formatted_lines = []

        for line in lines:
            line = line.strip()
            if not line:
                formatted_lines.append("")
                continue

            # Simple markdown formatting rules
            # Can extend with more rules as needed
            formatted_lines.append(line)

        return "\n".join(formatted_lines)

    def _extract_text_from_wps_stream(self, data: bytes) -> str:
        """Extract text from WPS WordDocument stream (lenient strategy)."""
        try:
            text_parts = []
            text_parts.extend(self._extract_unicode_blocks(data))
            text_parts.extend(self._extract_multibyte_blocks(data))

            if text_parts:
                combined = "\n".join(text_parts)
                return self._clean_extracted_text(combined)

            return self._parse_word_stream(data)

        except Exception as e:
            logger.error(f"üí• Failed to parse WPS stream: {str(e)}")
            return ""

    def _extract_unicode_blocks(self, data: bytes) -> list[str]:
        """Extract UTF-16-LE encoded ASCII text."""
        parts = []
        i = 0
        length = len(data)

        while i + 2 < length:
            if data[i + 1] == 0 and 32 <= data[i] <= 126:
                j, block = self._read_unicode_block(data, i)
                if len(block) > 10:
                    parts.append(block.decode("ascii", errors="ignore"))
                i = j
            else:
                i += 1

        return parts

    def _read_unicode_block(self, data: bytes, start: int) -> tuple[int, bytearray]:
        """Read continuous 16-bit little-endian ASCII block."""
        block = bytearray()
        j = start
        length = len(data)

        while j + 1 < length and data[j + 1] == 0 and 32 <= data[j] <= 126:
            block.append(data[j])
            j += 2

        return j, block

    def _extract_multibyte_blocks(self, data: bytes) -> list[str]:
        """Extract multibyte UTF-8/GBK/GB18030 blocks (mainly Chinese)."""
        parts = []
        i = 0
        length = len(data)

        while i < length:
            if self._looks_like_multibyte_start(data[i]):
                j, block = self._read_multibyte_block(data, i)
                decoded = self._try_decode_multibyte(block)
                if decoded:
                    parts.append(decoded)
                i = j
            else:
                i += 1

        return parts

    def _looks_like_multibyte_start(self, b: int) -> bool:
        """Heuristic for UTF-8 (E0‚ÄìEF) or GBK/GB18030 (81‚ÄìFE) leading bytes."""
        return 0xE0 <= b <= 0xEF or 0x81 <= b <= 0xFE

    def _read_multibyte_block(self, data: bytes, start: int) -> tuple[int, bytearray]:
        """Read a possible multibyte (UTF-8 / GBK / GB18030) text block."""
        block = bytearray()
        j = start
        length = len(data)

        while j < length:
            if data[j] < 32 and data[j] not in (9, 10, 13):  # skip control chars
                break
            block.append(data[j])
            j += 1

        return j, block

    def _try_decode_multibyte(self, block: bytearray) -> str | None:
        """Try decoding multibyte block using several encodings."""
        if len(block) <= 20:
            return None

        for encoding in ("utf-8", "gbk", "gb18030", "gb2312"):
            try:
                decoded = block.decode(encoding, errors="ignore")
                if decoded and len(decoded.strip()) > 10:
                    return decoded
            except Exception:
                continue

        return None
