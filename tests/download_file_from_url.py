import os
import requests
import mimetypes
# import magic  # éœ€è¦å®‰è£…python-magic
from urllib.parse import urlparse, parse_qs, unquote
import tempfile
import shutil
import base64
import zipfile  # ç”¨äºäºŒæ¬¡éªŒè¯ Office æ–‡æ¡£
import filetype
import re
from loguru import logger


def _handle_special_urls(url):
    """
    å¤„ç†ç‰¹æ®Šç±»å‹çš„URLï¼Œè¿”å›è§„èŒƒåŒ–çš„URLæˆ–ç‰¹æ®Šå¤„ç†æ ‡å¿—
    
    Args:
        url (str): åŸå§‹URL
        
    Returns:
        dict: åŒ…å«å¤„ç†ç»“æœçš„å­—å…¸
    """
    parsed = urlparse(url)
    domain = parsed.netloc.lower()
    
    # å¤„ç†å¸¸è§çš„äº‘å­˜å‚¨å’ŒCDNæœåŠ¡
    special_handling = {
        'needs_auth': False,
        'url': url,
        'headers': {},
        'method_preference': ['head', 'range', 'stream'],
        'storage_type': 'generic',
        'requires_special_download': False
    }
    
    # é˜¿é‡Œäº‘OSSæ£€æµ‹å’Œå¤„ç†
    if 'aliyuncs.com' in domain or 'oss-' in domain:
        logger.info(f"ğŸ” æ£€æµ‹åˆ°é˜¿é‡Œäº‘OSS URL")
        special_handling['storage_type'] = 'aliyun_oss'
        special_handling['requires_special_download'] = True
        
        # OSSç‰¹æ®Šè¯·æ±‚å¤´
        special_handling['headers'].update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': '*/*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache'
        })
        
        # OSSé€šå¸¸æ”¯æŒHEADè¯·æ±‚
        special_handling['method_preference'] = ['head', 'range', 'stream']
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºå†…ç½‘åŸŸå
        if '-internal.' in domain:
            logger.warning(f"âš ï¸ æ£€æµ‹åˆ°OSSå†…ç½‘åŸŸåï¼Œå¯èƒ½éœ€è¦VPCç¯å¢ƒè®¿é—®")
    
    # åä¸ºäº‘OBSæ£€æµ‹å’Œå¤„ç†
    elif 'obs.' in domain and 'myhuaweicloud.com' in domain:
        logger.info(f"ğŸ” æ£€æµ‹åˆ°åä¸ºäº‘OBS URL")
        special_handling['storage_type'] = 'huawei_obs'
        special_handling['requires_special_download'] = True
        
        special_handling['headers'].update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': '*/*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        })
        
        # OBSé€šå¸¸æ”¯æŒHEADè¯·æ±‚
        special_handling['method_preference'] = ['head', 'range', 'stream']
    
    # è…¾è®¯äº‘COSæ£€æµ‹å’Œå¤„ç†
    elif 'cos.' in domain and 'myqcloud.com' in domain:
        logger.info(f"ğŸ” æ£€æµ‹åˆ°è…¾è®¯äº‘COS URL")
        special_handling['storage_type'] = 'tencent_cos'
        special_handling['requires_special_download'] = True
        
        special_handling['headers'].update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': '*/*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        })
        
        special_handling['method_preference'] = ['head', 'range', 'stream']
    
    # AWS S3æ£€æµ‹å’Œå¤„ç†
    elif ('s3.amazonaws.com' in domain or 
          's3-' in domain or 
          '.s3.' in domain or 
          domain.endswith('.amazonaws.com')):
        logger.info(f"ğŸ” æ£€æµ‹åˆ°AWS S3 URL")
        special_handling['storage_type'] = 'aws_s3'
        special_handling['requires_special_download'] = True
        
        special_handling['headers'].update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': '*/*',
        })
        
        special_handling['method_preference'] = ['head', 'range', 'stream']
    
    # Google Cloud Storageæ£€æµ‹å’Œå¤„ç†
    elif 'storage.googleapis.com' in domain or 'storage.cloud.google.com' in domain:
        logger.info(f"ğŸ” æ£€æµ‹åˆ°Google Cloud Storage URL")
        special_handling['storage_type'] = 'google_gcs'
        special_handling['requires_special_download'] = True
        
        special_handling['headers'].update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': '*/*',
        })
        
        special_handling['method_preference'] = ['head', 'range', 'stream']
    
    # ä¸ƒç‰›äº‘æ£€æµ‹å’Œå¤„ç†
    elif any(provider in domain for provider in ['qiniu.com', 'qiniucdn.com', 'clouddn.com']):
        logger.info(f"ğŸ” æ£€æµ‹åˆ°ä¸ƒç‰›äº‘å­˜å‚¨URL")
        special_handling['storage_type'] = 'qiniu'
        special_handling['requires_special_download'] = True
        
        special_handling['headers'].update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': '*/*',
        })
        
        special_handling['method_preference'] = ['head', 'range', 'stream']
    
    # åˆæ‹äº‘æ£€æµ‹å’Œå¤„ç†
    elif any(provider in domain for provider in ['upyun.com', 'upaiyun.com']):
        logger.info(f"ğŸ” æ£€æµ‹åˆ°åˆæ‹äº‘å­˜å‚¨URL")
        special_handling['storage_type'] = 'upyun'
        special_handling['requires_special_download'] = True
        
        special_handling['headers'].update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': '*/*',
        })
        
        special_handling['method_preference'] = ['head', 'stream']  # åˆæ‹äº‘å¯èƒ½ä¸æ”¯æŒRange
    
    # Google Drive æ–‡ä»¶
    elif 'drive.google.com' in domain:
        logger.info(f"ğŸ” æ£€æµ‹åˆ°Google Drive URL")
        if '/file/d/' in url:
            file_id = url.split('/file/d/')[1].split('/')[0]
            # è½¬æ¢ä¸ºç›´æ¥ä¸‹è½½é“¾æ¥
            special_handling['url'] = f"https://drive.google.com/uc?export=download&id={file_id}"
            special_handling['headers']['User-Agent'] = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            special_handling['storage_type'] = 'google_drive'
    
    # OneDrive æ–‡ä»¶
    elif 'onedrive.live.com' in domain or '1drv.ms' in domain:
        logger.info(f"ğŸ” æ£€æµ‹åˆ°OneDrive URL")
        special_handling['headers']['User-Agent'] = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        special_handling['method_preference'] = ['stream']  # OneDrive é€šå¸¸ä¸æ”¯æŒHEAD
        special_handling['storage_type'] = 'onedrive'
    
    # Dropbox æ–‡ä»¶
    elif 'dropbox.com' in domain:
        logger.info(f"ğŸ” æ£€æµ‹åˆ°Dropbox URL")
        if 'dl=0' in url:
            special_handling['url'] = url.replace('dl=0', 'dl=1')
        special_handling['headers']['User-Agent'] = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        special_handling['storage_type'] = 'dropbox'
    
    # ç™¾åº¦ç½‘ç›˜ç­‰éœ€è¦ç‰¹æ®Šå¤„ç†çš„æœåŠ¡
    elif any(service in domain for service in ['pan.baidu.com', 'lanzous.com', 'lanzoux.com']):
        logger.warning(f"ğŸ”’ æ£€æµ‹åˆ°éœ€è¦èº«ä»½éªŒè¯çš„ç½‘ç›˜æœåŠ¡")
        special_handling['needs_auth'] = True
        special_handling['method_preference'] = ['stream']
        special_handling['storage_type'] = 'protected_drive'
    
    # GitHub releases/raw files
    elif 'github.com' in domain or 'githubusercontent.com' in domain:
        logger.info(f"ğŸ” æ£€æµ‹åˆ°GitHubæ–‡ä»¶URL")
        special_handling['headers']['Accept'] = 'application/octet-stream'
        special_handling['storage_type'] = 'github'
    
    # CDNæœåŠ¡
    elif any(cdn in domain for cdn in ['jsdelivr.net', 'unpkg.com', 'cdnjs.cloudflare.com']):
        logger.info(f"ğŸ” æ£€æµ‹åˆ°CDNæœåŠ¡URL")
        special_handling['method_preference'] = ['head', 'range', 'stream']
        special_handling['storage_type'] = 'cdn'
    
    # æ£€æŸ¥URLä¸­çš„ç‰¹æ®Šå‚æ•°ï¼ˆå¦‚ç­¾åä¿¡æ¯ï¼‰
    if parsed.query:
        query_params = parse_qs(parsed.query)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸´æ—¶ç­¾åå‚æ•°
        signature_params = ['signature', 'sig', 'token', 'x-oss-signature', 'x-cos-signature', 'Expires']
        has_signature = any(param in query_params for param in signature_params)
        
        if has_signature:
            logger.info(f"ğŸ” æ£€æµ‹åˆ°ç­¾åå‚æ•°ï¼Œå¯èƒ½ä¸ºä¸´æ—¶è®¿é—®URL")
            special_handling['has_signature'] = True
            
            # å¯¹äºæœ‰ç­¾åçš„URLï¼Œé€šå¸¸éœ€è¦å®Œæ•´ä¿ç•™å‚æ•°
            special_handling['preserve_query'] = True
    
    logger.debug(f"ğŸ”§ URLå¤„ç†ç»“æœ: {special_handling}")
    return special_handling


def get_remote_file_size(url):
    """
    è·å–è¿œç¨‹æ–‡ä»¶å¤§å°ï¼Œæ”¯æŒå„ç§ç±»å‹çš„URL
    
    Args:
        url (str): æ–‡ä»¶çš„URLåœ°å€
        
    Returns:
        int | None: æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰ï¼Œå¦‚æœæ— æ³•è·å–åˆ™è¿”å›None
    """
    import time
    import urllib3
    
    # æŠ‘åˆ¶SSLè­¦å‘Š
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    
    # å¤„ç†ç‰¹æ®ŠURLç±»å‹
    special_config = _handle_special_urls(url)
    actual_url = special_config['url']
    
    logger.info(f"ğŸ” å¼€å§‹è·å–æ–‡ä»¶å¤§å°")
    logger.info(f"ğŸ“ åŸå§‹URL: {url}")
    if actual_url != url:
        logger.info(f"ğŸ”„ è½¬æ¢åURL: {actual_url}")
    logger.info(f"âš™ï¸ ç‰¹æ®Šé…ç½®: {special_config}")
    
    if special_config['needs_auth']:
        logger.warning(f"ğŸ”’ URLå¯èƒ½éœ€è¦èº«ä»½éªŒè¯ï¼Œæ— æ³•ç›´æ¥è·å–æ–‡ä»¶å¤§å°: {url}")
        return None
    
    # é…ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': '*/*',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Cache-Control': 'max-age=0'
    }
    
    # åº”ç”¨ç‰¹æ®Šé…ç½®çš„è¯·æ±‚å¤´
    headers.update(special_config['headers'])
    logger.debug(f"ğŸ“‹ è¯·æ±‚å¤´é…ç½®: {headers}")
    
    # é…ç½®ä¼šè¯
    session = requests.Session()
    session.headers.update(headers)
    
    # è®¾ç½®é‡è¯•å‚æ•°
    max_retries = 3
    retry_delay = 1
    timeout = 15
    
    # æ ¹æ®ç‰¹æ®Šé…ç½®ç¡®å®šæ–¹æ³•ä¼˜å…ˆçº§
    method_preference = special_config.get('method_preference', ['head', 'range', 'stream'])
    logger.info(f"ğŸ¯ æ–¹æ³•ä¼˜å…ˆçº§: {method_preference}")
    
    for attempt in range(max_retries):
        try:
            logger.info(f"ğŸš€ å°è¯•è·å–æ–‡ä»¶å¤§å° (ç¬¬ {attempt + 1}/{max_retries} æ¬¡)")
            
            # æŒ‰ç…§ä¼˜å…ˆçº§å°è¯•ä¸åŒæ–¹æ³•
            for method_idx, method in enumerate(method_preference):
                logger.info(f"ğŸ”§ ä½¿ç”¨æ–¹æ³• {method_idx + 1}/{len(method_preference)}: {method.upper()}")
                
                if method == 'head':
                    # 1. é¦–å…ˆå°è¯•HEADè¯·æ±‚
                    try:
                        start_time = time.time()
                        response = session.head(
                            actual_url, 
                            allow_redirects=True, 
                            timeout=timeout,
                            verify=False  # å¿½ç•¥SSLè¯ä¹¦éªŒè¯é—®é¢˜
                        )
                        request_time = time.time() - start_time
                        
                        logger.info(f"ğŸ“¡ HEADè¯·æ±‚å®Œæˆ - çŠ¶æ€ç : {response.status_code}, è€—æ—¶: {request_time:.2f}s")
                        logger.debug(f"ğŸ“‹ å“åº”å¤´: {dict(response.headers)}")
                        
                        # æ£€æŸ¥å“åº”çŠ¶æ€ç 
                        if 200 <= response.status_code < 300:
                            content_length = response.headers.get('Content-Length')
                            if content_length and content_length.isdigit():
                                size = int(content_length)
                                size_mb = size / (1024 * 1024)
                                logger.success(f"âœ… é€šè¿‡HEADè¯·æ±‚è·å–åˆ°æ–‡ä»¶å¤§å°: {size:,} å­—èŠ‚ ({size_mb:.2f} MB)")
                                return size
                            
                            # æ£€æŸ¥æ˜¯å¦ä¸ºç½‘é¡µå†…å®¹
                            content_type = response.headers.get('Content-Type', '').lower()
                            if any(ct in content_type for ct in ['text/html', 'application/json', 'text/plain']):
                                logger.info(f"ğŸŒ æ£€æµ‹åˆ°ç½‘é¡µå†…å®¹ï¼ŒContent-Type: {content_type}")
                                continue  # å°è¯•ä¸‹ä¸€ä¸ªæ–¹æ³•
                            else:
                                logger.warning(f"âš ï¸ HEADè¯·æ±‚æ— Content-Lengthå¤´ï¼ŒContent-Type: {content_type}")
                        else:
                            logger.warning(f"âš ï¸ HEADè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                                
                    except requests.exceptions.RequestException as e:
                        logger.warning(f"âŒ HEADè¯·æ±‚å¼‚å¸¸: {type(e).__name__}: {e}")
                        continue
                
                elif method == 'range':
                    # 2. ä½¿ç”¨Rangeè¯·æ±‚è·å–éƒ¨åˆ†å†…å®¹
                    try:
                        range_headers = headers.copy()
                        range_headers['Range'] = 'bytes=0-0'
                        
                        start_time = time.time()
                        range_response = session.get(
                            actual_url, 
                            headers=range_headers,
                            timeout=timeout,
                            verify=False
                        )
                        request_time = time.time() - start_time
                        
                        logger.info(f"ğŸ“¡ Rangeè¯·æ±‚å®Œæˆ - çŠ¶æ€ç : {range_response.status_code}, è€—æ—¶: {request_time:.2f}s")
                        logger.debug(f"ğŸ“‹ Rangeå“åº”å¤´: {dict(range_response.headers)}")
                        
                        if range_response.status_code == 206:  # Partial Content
                            content_range = range_response.headers.get('Content-Range')
                            if content_range:
                                logger.debug(f"ğŸ“ Content-Range: {content_range}")
                                # è§£æ Content-Range: bytes 0-0/total_size
                                match = re.search(r'bytes \d+-\d+/(\d+)', content_range)
                                if match:
                                    total_size = int(match.group(1))
                                    size_mb = total_size / (1024 * 1024)
                                    logger.success(f"âœ… é€šè¿‡Rangeè¯·æ±‚è·å–åˆ°æ–‡ä»¶å¤§å°: {total_size:,} å­—èŠ‚ ({size_mb:.2f} MB)")
                                    return total_size
                                else:
                                    logger.warning(f"âš ï¸ æ— æ³•è§£æContent-Range: {content_range}")
                            else:
                                logger.warning(f"âš ï¸ Rangeå“åº”ç¼ºå°‘Content-Rangeå¤´")
                        else:
                            logger.warning(f"âš ï¸ Rangeè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {range_response.status_code}")
                            
                    except requests.exceptions.RequestException as e:
                        logger.warning(f"âŒ Rangeè¯·æ±‚å¼‚å¸¸: {type(e).__name__}: {e}")
                        continue
                
                elif method == 'stream':
                    # 3. ä½¿ç”¨æµå¼GETè¯·æ±‚
                    try:
                        start_time = time.time()
                        response = session.get(
                            actual_url, 
                            stream=True, 
                            allow_redirects=True, 
                            timeout=timeout,
                            verify=False
                        )
                        response.raise_for_status()
                        request_time = time.time() - start_time
                        
                        logger.info(f"ğŸ“¡ æµå¼GETè¯·æ±‚å®Œæˆ - çŠ¶æ€ç : {response.status_code}, è€—æ—¶: {request_time:.2f}s")
                        logger.debug(f"ğŸ“‹ GETå“åº”å¤´: {dict(response.headers)}")
                        
                        # éªŒè¯Content-Typeï¼Œé¿å…å°†é”™è¯¯é¡µé¢å½“ä½œæ–‡ä»¶
                        content_type = response.headers.get('Content-Type', '').lower()
                        logger.debug(f"ğŸ“„ Content-Type: {content_type}")
                        
                        # å¯¹äºå¯¹è±¡å­˜å‚¨ï¼Œæ£€æŸ¥æ˜¯å¦è¿”å›äº†é”™è¯¯å“åº”
                        storage_type = special_config.get('storage_type', 'generic')
                        if storage_type in ['aliyun_oss', 'huawei_obs', 'tencent_cos', 'aws_s3', 'google_gcs']:
                            # æ£€æŸ¥æ˜¯å¦ä¸ºXMLé”™è¯¯å“åº”
                            if 'xml' in content_type or 'html' in content_type or 'json' in content_type:
                                logger.warning(f"âš ï¸ å¯¹è±¡å­˜å‚¨è¿”å›äº†é”™è¯¯æ ¼å¼: {content_type}")
                                
                                # è¯»å–éƒ¨åˆ†å†…å®¹è¿›è¡ŒéªŒè¯
                                try:
                                    peek_size = min(1024, int(response.headers.get('Content-Length', 1024)))
                                    peek_content = b''
                                    for chunk in response.iter_content(chunk_size=peek_size):
                                        peek_content += chunk
                                        if len(peek_content) >= peek_size:
                                            break
                                    
                                    # å°è¯•è§£ç æŸ¥çœ‹å†…å®¹
                                    try:
                                        text_content = peek_content.decode('utf-8', errors='ignore')
                                        logger.debug(f"ğŸ“„ å“åº”å†…å®¹é¢„è§ˆ: {text_content[:200]}...")
                                        
                                        # æ£€æŸ¥å¯¹è±¡å­˜å‚¨çš„å…¸å‹é”™è¯¯å“åº”
                                        error_patterns = [
                                            'accessdenied', 'access denied', 'nosuchkey', 'not found',
                                            'invalidrequest', 'signaturemismatch', 'requesttimeout',
                                            'error', 'exception', '<html', '<!doctype'
                                        ]
                                        
                                        text_lower = text_content.lower()
                                        if any(pattern in text_lower for pattern in error_patterns):
                                            logger.error(f"âŒ æ£€æµ‹åˆ°å¯¹è±¡å­˜å‚¨é”™è¯¯å“åº”")
                                            if 'access' in text_lower and 'denied' in text_lower:
                                                logger.error(f"ğŸš« è®¿é—®è¢«æ‹’ç»ï¼Œå¯èƒ½æ˜¯æƒé™é—®é¢˜æˆ–ç­¾åè¿‡æœŸ")
                                            elif 'not found' in text_lower or 'nosuchkey' in text_lower:
                                                logger.error(f"ğŸš« æ–‡ä»¶ä¸å­˜åœ¨")
                                            else:
                                                logger.error(f"ğŸš« å…¶ä»–é”™è¯¯: {text_content[:100]}...")
                                            return None
                                            
                                    except UnicodeDecodeError:
                                        # å¦‚æœæ— æ³•è§£ç ï¼Œå¯èƒ½æ˜¯äºŒè¿›åˆ¶æ–‡ä»¶ï¼Œç»§ç»­å¤„ç†
                                        logger.debug(f"ğŸ“„ æ— æ³•è§£ç å“åº”å†…å®¹ï¼Œå¯èƒ½æ˜¯äºŒè¿›åˆ¶æ–‡ä»¶")
                                        pass
                                        
                                except Exception as e:
                                    logger.warning(f"âš ï¸ å†…å®¹éªŒè¯å¤±è´¥: {e}")
                        
                        # æ£€æŸ¥Content-Lengthå¤´
                        content_length = response.headers.get('Content-Length')
                        if content_length and content_length.isdigit():
                            size = int(content_length)
                            size_mb = size / (1024 * 1024)
                            logger.success(f"âœ… é€šè¿‡GETè¯·æ±‚è·å–åˆ°æ–‡ä»¶å¤§å°: {size:,} å­—èŠ‚ ({size_mb:.2f} MB)")
                            return size
                        
                        # æ£€æŸ¥Transfer-Encoding
                        transfer_encoding = response.headers.get('Transfer-Encoding')
                        if transfer_encoding == 'chunked':
                            logger.warning(f"ğŸ”„ æœåŠ¡å™¨ä½¿ç”¨åˆ†å—ä¼ è¾“ï¼Œæ— æ³•é¢„å…ˆè·å–æ–‡ä»¶å¤§å°")
                            
                            # å¯¹äºåˆ†å—ä¼ è¾“ï¼Œå¯ä»¥å°è¯•ä¸‹è½½ä¸€å°éƒ¨åˆ†æ¥ä¼°ç®—
                            if any(ct in content_type for ct in ['text/', 'application/json', 'application/xml']):
                                logger.info(f"ğŸ“ æ–‡æœ¬ç±»å‹æ–‡ä»¶ï¼Œå°è¯•é‡‡æ ·ä¼°ç®—å¤§å°")
                                # å¯¹äºæ–‡æœ¬ç±»å‹æ–‡ä»¶ï¼Œä¸‹è½½ä¸€éƒ¨åˆ†å†…å®¹æ¥ä¼°ç®—
                                chunk_size = 8192
                                downloaded = 0
                                sample_chunks = []
                                
                                for chunk in response.iter_content(chunk_size=chunk_size):
                                    if chunk:
                                        sample_chunks.append(chunk)
                                        downloaded += len(chunk)
                                        if downloaded >= chunk_size * 5:  # ä¸‹è½½5ä¸ªå—ä½œä¸ºæ ·æœ¬
                                            break
                                
                                if sample_chunks:
                                    # ç®€å•ä¼°ç®—ï¼šå‡è®¾å¹³å‡å—å¤§å°
                                    avg_chunk_size = downloaded / len(sample_chunks)
                                    logger.info(f"ğŸ“Š åŸºäºæ ·æœ¬ä¼°ç®—ï¼Œå¹³å‡å—å¤§å°: {avg_chunk_size:.0f} å­—èŠ‚ï¼Œé‡‡æ ·: {downloaded:,} å­—èŠ‚")
                                    # è¿”å›Noneè¡¨ç¤ºæ— æ³•ç¡®å®šå¤§å°ï¼Œä½†æ–‡ä»¶æ˜¯å¯è®¿é—®çš„
                                    return None
                            else:
                                logger.warning(f"â“ æ— æ³•ç¡®å®šåˆ†å—ä¼ è¾“æ–‡ä»¶çš„å¤§å°ï¼ŒContent-Type: {content_type}")
                                return None
                        
                        # å¦‚æœæ—¢æ²¡æœ‰Content-Lengthä¹Ÿä¸æ˜¯chunkedï¼Œå°è¯•è¯»å–å®Œæ•´å“åº”
                        logger.warning(f"â“ æ—¢æ²¡æœ‰Content-Lengthä¹Ÿä¸æ˜¯åˆ†å—ä¼ è¾“ï¼Œæ— æ³•ç¡®å®šæ–‡ä»¶å¤§å°")
                        return None
                        
                    except requests.exceptions.RequestException as e:
                        logger.error(f"âŒ GETè¯·æ±‚å¼‚å¸¸: {type(e).__name__}: {e}")
                        continue
            
            # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥äº†ï¼Œè¿›è¡Œé‡è¯•
            if attempt < max_retries - 1:
                logger.warning(f"ğŸ”„ æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                time.sleep(retry_delay)
                retry_delay *= 2  # æŒ‡æ•°é€€é¿
                continue
            else:
                logger.error(f"ğŸ’¥ æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†")
                return None
                     
        except Exception as e:
            logger.error(f"ğŸ’¥ è·å–æ–‡ä»¶å¤§å°æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {type(e).__name__}: {e}")
            if attempt < max_retries - 1:
                logger.info(f"ğŸ”„ ç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                time.sleep(retry_delay)
                retry_delay *= 2
                continue
            else:
                return None
    
    logger.error(f"ğŸ’¥ æ— æ³•è·å–æ–‡ä»¶å¤§å°: {url}")
    return None


def save_base64_file(base64_data: str,
                     save_dir: str,
                     file_type: str = None,
                     save_file_name: str = "download_file"):
    """
    ä¿å­˜ base64 æ–‡ä»¶ï¼Œå¹¶ç¡®ä¿æ‰©å±•åæ­£ç¡®
    - ä¼˜å…ˆé€šè¿‡ MIME ç±»å‹æ£€æµ‹
    - å¦‚æœæ˜¯ .bin/.unknownï¼Œå°è¯•é€šè¿‡æ–‡ä»¶å†…å®¹äºŒæ¬¡éªŒè¯
    """
    try:
        file_data = base64.b64decode(base64_data)
        os.makedirs(save_dir, exist_ok=True)

        if not file_type:
            # ç”¨æˆ·å¦‚æœæ²¡æœ‰æä¾›æ–‡ä»¶ç±»å‹ è¿›è¡Œguess ä½†ä¸ç¡®ä¿æ­£ç¡®æˆ–æ— æ³•è¯†åˆ«
            kind = filetype.guess(file_data)
            if kind is not None:
                file_type = f".{kind.extension}"
            else:
                file_type = ".unknown"
        else:
            file_type = f".{file_type.lstrip('.')}"


        # æœ€ç»ˆæ–‡ä»¶å
        final_name = f"{save_file_name}{file_type}"
        final_path = os.path.join(save_dir, final_name)
        with open(final_path, "wb") as f:
            f.write(file_data)

        return {
            "success": True,
            "file_path": final_path,
            "file_type": file_type,
            "message": "æ–‡ä»¶ä¿å­˜æˆåŠŸ"
        }
    except Exception as e:
        return {
            "success": False,
            "message": f"ä¿å­˜å¤±è´¥: {str(e)}"
        }


def download_file(
        url,
        save_dir='.',
        file_type: str = None,
        save_file_name="download_file"
):
    """
    å¢å¼ºçš„æ–‡ä»¶ä¸‹è½½å‡½æ•°ï¼Œæ”¯æŒå¯¹è±¡å­˜å‚¨ç­‰ç‰¹æ®ŠURLç±»å‹
    """
    temp_path = None
    try:
        # æ£€æŸ¥æ˜¯å¦ä¸ºå¯¹è±¡å­˜å‚¨ç­‰ç‰¹æ®ŠURL
        special_config = _handle_special_urls(url)
        
        # ä½¿ç”¨ç‰¹æ®Šé…ç½®çš„URLå’Œè¯·æ±‚å¤´
        actual_url = special_config['url']
        storage_type = special_config.get('storage_type', 'generic')
        
        # åŸºç¡€è¯·æ±‚å¤´
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': '*/*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        }
        
        # åº”ç”¨ç‰¹æ®Šé…ç½®çš„è¯·æ±‚å¤´
        headers.update(special_config['headers'])
        
        print(f"ğŸ“¡ å¼€å§‹ä¸‹è½½æ–‡ä»¶")
        print(f"ğŸ”— URL: {actual_url}")
        print(f"ğŸ·ï¸ å­˜å‚¨ç±»å‹: {storage_type}")
        print(f"ğŸ“‹ è¯·æ±‚å¤´: {headers}")
        
        # å¯¹è±¡å­˜å‚¨ç‰¹æ®Šå¤„ç†
        session = requests.Session()
        session.headers.update(headers)
        
        # é…ç½®é‡è¯•å’Œè¶…æ—¶
        timeout = 30
        max_retries = 3
        
        for attempt in range(max_retries):
            try:
                print(f"ğŸš€ å°è¯•ä¸‹è½½ (ç¬¬ {attempt + 1}/{max_retries} æ¬¡)")
                
                # å‘é€HTTP GETè¯·æ±‚
                response = session.get(
                    actual_url, 
                    stream=True, 
                    allow_redirects=True,
                    timeout=timeout,
                    verify=False  # å¯¹äºå†…éƒ¨æˆ–æµ‹è¯•ç¯å¢ƒå¯èƒ½éœ€è¦
                )
                
                print(f"ğŸ“¡ å“åº”çŠ¶æ€ç : {response.status_code}")
                print(f"ğŸ“‹ å“åº”å¤´: {dict(response.headers)}")
                
                # æ£€æŸ¥å“åº”çŠ¶æ€
                if response.status_code == 200:
                    break
                elif response.status_code == 403:
                    print(f"ğŸš« è®¿é—®è¢«æ‹’ç» (403) - å¯èƒ½éœ€è¦èº«ä»½éªŒè¯æˆ–URLå·²è¿‡æœŸ")
                    if attempt == max_retries - 1:
                        raise Exception(f"è®¿é—®è¢«æ‹’ç»: HTTP {response.status_code}")
                elif response.status_code == 404:
                    print(f"ğŸš« æ–‡ä»¶ä¸å­˜åœ¨ (404)")
                    raise Exception(f"æ–‡ä»¶ä¸å­˜åœ¨: HTTP {response.status_code}")
                elif response.status_code >= 500:
                    print(f"ğŸš« æœåŠ¡å™¨é”™è¯¯ ({response.status_code}) - å°è¯•é‡è¯•")
                    if attempt == max_retries - 1:
                        raise Exception(f"æœåŠ¡å™¨é”™è¯¯: HTTP {response.status_code}")
                else:
                    print(f"âš ï¸ æ„å¤–çŠ¶æ€ç : {response.status_code}")
                    if attempt == max_retries - 1:
                        raise Exception(f"è¯·æ±‚å¤±è´¥: HTTP {response.status_code}")
                        
                # ç­‰å¾…é‡è¯•
                import time
                wait_time = 2 ** attempt
                print(f"â° ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                time.sleep(wait_time)
                
            except requests.exceptions.Timeout:
                print(f"â° è¯·æ±‚è¶…æ—¶")
                if attempt == max_retries - 1:
                    raise Exception("è¯·æ±‚è¶…æ—¶")
            except requests.exceptions.ConnectionError:
                print(f"ğŸ”Œ è¿æ¥é”™è¯¯")
                if attempt == max_retries - 1:
                    raise Exception("è¿æ¥å¤±è´¥")
            except Exception as e:
                print(f"âŒ è¯·æ±‚å¼‚å¸¸: {e}")
                if attempt == max_retries - 1:
                    raise

        # éªŒè¯Content-Typeï¼Œé¿å…ä¸‹è½½åˆ°é”™è¯¯å†…å®¹
        content_type = response.headers.get('Content-Type', '').lower()
        print(f"ğŸ“„ Content-Type: {content_type}")
        
        # ä¿®æ”¹åçš„é”™è¯¯æ£€æµ‹é€»è¾‘
        if any(ct in content_type for ct in ['text/html', 'application/json', 'text/xml']):
            first_chunk = next(response.iter_content(chunk_size=1024), b'')
            try:
                text_content = first_chunk.decode('utf-8', errors='ignore')
                
                # æ–°å¢ï¼šæ£€æŸ¥æ˜¯å¦ä¸ºå¯¹è±¡å­˜å‚¨çš„é”™è¯¯å“åº”
                is_object_storage_error = (
                    storage_type in ['aliyun_oss', 'huawei_obs', 'tencent_cos', 'aws_s3'] and
                    any(indicator in text_content.lower() 
                        for indicator in ['error', 'exception', 'accessdenied', 'nosuchkey'])
                )
                
                # ä»…å½“ç¡®è®¤æ˜¯å¯¹è±¡å­˜å‚¨é”™è¯¯æ—¶æ‰æŠ›å‡ºå¼‚å¸¸
                if is_object_storage_error:
                    if 'access denied' in text_content.lower() or 'accessdenied' in text_content.lower():
                        raise Exception(f"å¯¹è±¡å­˜å‚¨è®¿é—®è¢«æ‹’ç»")
                    elif 'not found' in text_content.lower() or 'nosuchkey' in text_content.lower():
                        raise Exception(f"æ–‡ä»¶ä¸å­˜åœ¨")
                    else:
                        raise Exception(f"å¯¹è±¡å­˜å‚¨é”™è¯¯: {text_content[:100]}...")
                        
                # å¦åˆ™è®°å½•è­¦å‘Šä½†ç»§ç»­ä¸‹è½½
                logger.warning(f"âš ï¸ æ£€æµ‹åˆ°æ–‡æœ¬ç±»å‹å†…å®¹ï¼Œä½†ç»§ç»­ä¸‹è½½: {content_type}")

            except UnicodeDecodeError:
                pass
            
            # é‡æ–°åˆ›å»ºå“åº”å¯¹è±¡ä»¥åŒ…å«å·²è¯»å–çš„chunk
            import io
            content = first_chunk + response.content
            response = type('MockResponse', (), {
                'headers': response.headers,
                'iter_content': lambda chunk_size=8192: [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]
            })()

        # ------------------ æ‰©å±•åå¤„ç†é€»è¾‘ä¼˜åŒ– ------------------
        # æ ‡å‡†åŒ–ç”¨æˆ·æä¾›çš„æ‰©å±•åï¼ˆç¡®ä¿æœ‰å‰å¯¼ç‚¹ï¼‰
        if file_type:
            extension = f".{file_type.lstrip('.')}" if file_type else None
        else:
            # è‡ªåŠ¨æ£€æµ‹æ‰©å±•åçš„é€»è¾‘
            extension = None
            mime_type = content_type.split(';')[0].strip()

            # æ¥æº1: Content-Typeçš„MIMEç±»å‹
            if mime_type and mime_type != 'application/octet-stream':
                extension = mimetypes.guess_extension(mime_type)
                logger.info(f"é€šè¿‡Content-Typeæ£€æµ‹åˆ°æ‰©å±•å: {extension}")

            # æ¥æº2: URLè·¯å¾„ä¸­çš„æ‰©å±•å
            if not extension:
                path = urlparse(actual_url).path  # ä½¿ç”¨å¤„ç†åçš„URL
                url_extension = os.path.splitext(path)[1]
                extension = url_extension.lower() if url_extension else None
                logger.info(f"URLè·¯å¾„ä¸­çš„æ‰©å±•å: {extension}")
                
            # æ¥æº3: æ–‡ä»¶å†…å®¹æ£€æµ‹ï¼ˆå‰ä¸¤ç§æ–¹å¼å¤±è´¥æ—¶ä½¿ç”¨ï¼‰
            if not extension:
                try:
                    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
                    fd, temp_path = tempfile.mkstemp()
                    os.close(fd)

                    # æµå¼ä¸‹è½½åˆ°ä¸´æ—¶æ–‡ä»¶
                    with open(temp_path, 'wb') as tmp_file:
                        for chunk in response.iter_content(chunk_size=8192):
                            if chunk:
                                tmp_file.write(chunk)

                    # ä½¿ç”¨magicæ£€æµ‹MIMEç±»å‹
                    try:
                        import magic
                        mime = magic.Magic(mime=True)
                        detected_mime = mime.from_file(temp_path)
                        extension = mimetypes.guess_extension(detected_mime) or '.unknown'
                        print(f"ğŸ” é€šè¿‡æ–‡ä»¶å†…å®¹æ£€æµ‹åˆ°æ‰©å±•å: {extension}")
                    except (ImportError, Exception) as e:
                        print(f"âš ï¸ æ–‡ä»¶å†…å®¹æ£€æµ‹å¤±è´¥: {e}")
                        extension = '.unknown'
                        
                except Exception as e:
                    print(f"âŒ åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
                    extension = '.unknown'

        # ç¡®ä¿æ‰©å±•åä¸ä¸ºç©º
        if not extension:
            extension = '.unknown'

        # ------------------ æ–‡ä»¶ä¿å­˜é€»è¾‘ ------------------
        # ç”Ÿæˆæœ€ç»ˆè·¯å¾„
        save_path = os.path.join(save_dir, f"{save_file_name}{extension}")
        os.makedirs(save_dir, exist_ok=True)

        # å¦‚æœå·²ä¸‹è½½åˆ°ä¸´æ—¶æ–‡ä»¶ï¼Œç›´æ¥ç§»åŠ¨ï¼›å¦åˆ™é‡æ–°ä¸‹è½½
        if temp_path and os.path.exists(temp_path):
            shutil.move(temp_path, save_path)
            print(f"âœ… æ–‡ä»¶å·²ä»ä¸´æ—¶ä½ç½®ç§»åŠ¨åˆ°: {save_path}")
        else:
            print(f"ğŸ’¾ å¼€å§‹ä¿å­˜æ–‡ä»¶åˆ°: {save_path}")
            with open(save_path, 'wb') as f:
                if extension in ['.html', '.json', '.xml']:
                    f.write(content)
                    total_size = len(content) 
                else:
                    total_size = 0
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                            total_size += len(chunk)
                        
                print(f"ğŸ’¾ æ–‡ä»¶ä¸‹è½½å®Œæˆï¼Œæ€»å¤§å°: {total_size} å­—èŠ‚")

        # éªŒè¯ä¸‹è½½çš„æ–‡ä»¶
        if os.path.exists(save_path):
            file_size = os.path.getsize(save_path)
            if file_size == 0:
                print(f"âš ï¸ ä¸‹è½½çš„æ–‡ä»¶å¤§å°ä¸º0ï¼Œå¯èƒ½ä¸‹è½½å¤±è´¥")
                os.remove(save_path)
                raise Exception("ä¸‹è½½çš„æ–‡ä»¶ä¸ºç©º")
            else:
                print(f"âœ… æ–‡ä»¶ä¸‹è½½æˆåŠŸï¼Œå¤§å°: {file_size} å­—èŠ‚")

        return {
            "success": True,
            "file_path": save_path,
            "file_type": extension,
            "storage_type": storage_type,
            "message": "æ–‡ä»¶ä¸‹è½½æˆåŠŸ"
        }

    except requests.exceptions.RequestException as e:
        print(f"âŒ è¯·æ±‚é”™è¯¯: {e}")
        return {"success": False, "error": str(e), "message": f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}"}
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return {"success": False, "error": str(e), "message": f"æ–‡ä»¶ä¸‹è½½å¤±è´¥: {str(e)}"}
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if temp_path and os.path.exists(temp_path):
            try:
                os.remove(temp_path)
                print(f"ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {temp_path}")
            except Exception as e:
                print(f"âš ï¸ ä¸´æ—¶æ–‡ä»¶æ¸…ç†å¤±è´¥: {e}")


# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    ...
    import requests
    import os

    # æ–‡ä»¶URL
    file_url = "https://obsv3.coscoshipping-shdc-1.ex.cloud.coscoshipping.com/data-factory-test/fty/parsing/202508/%E5%B7%B2%E8%A7%A3%E6%9E%90_2019%20(50Mb)_1njfTQNhXfG.XLSX?AWSAccessKeyId=ZBS3SXIKXNFRMAFOQFVV&Expires=1756175086&Signature=JkWSkurJddZrRVqSXMYjDDOA7Qg%3D"

    # æœ¬åœ°ä¿å­˜è·¯å¾„
    save_path = "downloaded_file.xlsx"

    # ä¸‹è½½æ–‡ä»¶
    response = requests.get(file_url, stream=True)
    response.raise_for_status()

    with open(save_path, 'wb') as f:
        for chunk in response.iter_content(chunk_size=8192):
            f.write(chunk)

    print(f"æ–‡ä»¶å·²ä¸‹è½½åˆ°: {os.path.abspath(save_path)}")
    # def download_file(
    #         url,
    #         save_dir='.',
    #         file_type: str = None,
    #         save_file_name="download_file"
    # ):


    # result = download_file(
    #     "https://arxiv.org/pdf/2501.12372",
    #     save_dir="output",
    #     save_file_name="download_file"
    # )
    # print(result, 111)
    # if result['success']:
    #     print(f"æ–‡ä»¶å·²ä¿å­˜è‡³: {result['file_path']}")
    #     print(f"æ£€æµ‹åˆ°çš„æ–‡ä»¶ç±»å‹: {result['file_type']}")
    # else:
    #     print(f"ä¸‹è½½å¤±è´¥: {result['message']}")


    # result = download_file(
    #     "https://arxiv.org/pdf/2501.12372",
    #     save_dir="output",
    #     save_file_name="download_file"
    # )
    # print(result, 111)
    # if result['success']:
    #     print(f"æ–‡ä»¶å·²ä¿å­˜è‡³: {result['file_path']}")
    #     print(f"æ£€æµ‹åˆ°çš„æ–‡ä»¶ç±»å‹: {result['file_type']}")
    # else:
    #     print(f"ä¸‹è½½å¤±è´¥: {result['message']}")